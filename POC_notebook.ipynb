{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake Social Media Account Detection – Proof of Concept (POC)\n",
    "by Hemanth Vignesh, Srujitha Jasmine \n",
    "**Goal:**  \n",
    "Build and evaluate multiple supervised learning models to detect **fake social media accounts** using a custom dataset (`fake_dataset.xlsx`).\n",
    "\n",
    "This POC will:\n",
    "1. Load and explore the dataset  \n",
    "2. Perform basic data cleaning and preprocessing  \n",
    "3. Train and compare different classification models  \n",
    "4. Select a final model (Gradient Boosting) based on performance  \n",
    "5. Summarize insights for deployment (used later in `train_model.py` and Streamlit app)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    roc_auc_score\n",
    ")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust path if needed (assumes notebook and file in same folder)\n",
    "DATA_PATH = \"fake_dataset.xlsx\"\n",
    "\n",
    "df = pd.read_excel(DATA_PATH)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape:\", df.shape)\n",
    "print(\"\\nColumns:\", df.columns.tolist())\n",
    "\n",
    "print(\"\\nInfo:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to detect the target column (same as app/train script logic)\n",
    "possible_targets = [\"is_fake\", \"label\", \"fake\", \"target\", \"class\", \"isbot\", \"bot\"]\n",
    "target_col = None\n",
    "\n",
    "for c in df.columns:\n",
    "    if c.lower() in possible_targets:\n",
    "        target_col = c\n",
    "        break\n",
    "\n",
    "if target_col is None:\n",
    "    # fallback: any column with exactly 2 unique values\n",
    "    for c in df.columns:\n",
    "        if df[c].dropna().nunique() == 2:\n",
    "            target_col = c\n",
    "            break\n",
    "\n",
    "print(\"Detected target column:\", target_col)\n",
    "\n",
    "df[target_col].value_counts().plot(kind=\"bar\")\n",
    "plt.title(f\"Class distribution for '{target_col}'\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.copy()\n",
    "\n",
    "# Drop high-cardinality or non-useful text columns explicitly\n",
    "cols_to_drop = []\n",
    "\n",
    "# 'username' is usually too unique → remove\n",
    "if \"username\" in df_clean.columns:\n",
    "    cols_to_drop.append(\"username\")\n",
    "\n",
    "# drop any obvious IDs if present\n",
    "id_like = [\"id\", \"user_id\", \"account_id\", \"handle\", \"uuid\"]\n",
    "for c in df_clean.columns:\n",
    "    if any(k in c.lower() for k in id_like):\n",
    "        cols_to_drop.append(c)\n",
    "\n",
    "cols_to_drop = list(set(cols_to_drop))\n",
    "print(\"Dropping columns:\", cols_to_drop)\n",
    "\n",
    "df_clean = df_clean.drop(columns=cols_to_drop, errors=\"ignore\")\n",
    "\n",
    "# Handle missing values (simple strategy, different from production pipeline)\n",
    "numeric_cols = df_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = df_clean.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "print(\"\\nNumeric columns:\", numeric_cols)\n",
    "print(\"Categorical columns:\", categorical_cols)\n",
    "\n",
    "# Fill numeric NaNs with median\n",
    "df_clean[numeric_cols] = df_clean[numeric_cols].fillna(df_clean[numeric_cols].median())\n",
    "\n",
    "# Fill categorical NaNs with mode\n",
    "for c in categorical_cols:\n",
    "    if df_clean[c].isnull().any():\n",
    "        df_clean[c] = df_clean[c].fillna(df_clean[c].mode()[0])\n",
    "\n",
    "df_clean.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and y\n",
    "X = df_clean.drop(columns=[target_col])\n",
    "y = df_clean[target_col]\n",
    "\n",
    "# If target is not numeric (e.g., \"Fake\"/\"Real\"), convert to 0/1\n",
    "if y.dtype == \"O\":\n",
    "    print(\"Converting string labels to 0/1...\")\n",
    "    y = y.map(lambda v: 1 if str(v).lower() in [\"fake\", \"1\", \"yes\", \"true\"] else 0)\n",
    "\n",
    "print(\"y value counts:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# One-hot encode categorical columns (e.g., platform)\n",
    "X_encoded = pd.get_dummies(X, drop_first=True)\n",
    "print(\"Shape after encoding:\", X_encoded.shape)\n",
    "\n",
    "X_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y  # good for classification\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "# Only scale numeric columns (all are numeric after get_dummies)\n",
    "X_train_scaled.loc[:, :] = scaler.fit_transform(X_train)\n",
    "X_test_scaled.loc[:, :] = scaler.transform(X_test)\n",
    "\n",
    "X_train_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "# 1. Logistic Regression\n",
    "log_reg = LogisticRegression(\n",
    "    max_iter=500,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "y_pred_lr = log_reg.predict(X_test_scaled)\n",
    "\n",
    "results[\"LogisticRegression\"] = {\n",
    "    \"accuracy\": accuracy_score(y_test, y_pred_lr),\n",
    "    \"precision\": precision_score(y_test, y_pred_lr, zero_division=0),\n",
    "    \"recall\": recall_score(y_test, y_pred_lr, zero_division=0),\n",
    "    \"f1\": f1_score(y_test, y_pred_lr, zero_division=0),\n",
    "}\n",
    "\n",
    "# 2. Random Forest\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "results[\"RandomForest\"] = {\n",
    "    \"accuracy\": accuracy_score(y_test, y_pred_rf),\n",
    "    \"precision\": precision_score(y_test, y_pred_rf, zero_division=0),\n",
    "    \"recall\": recall_score(y_test, y_pred_rf, zero_division=0),\n",
    "    \"f1\": f1_score(y_test, y_pred_rf, zero_division=0),\n",
    "}\n",
    "\n",
    "# 3. XGBoost\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=4,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=RANDOM_STATE,\n",
    "    eval_metric=\"logloss\"\n",
    ")\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb.predict(X_test)\n",
    "\n",
    "results[\"XGBoost\"] = {\n",
    "    \"accuracy\": accuracy_score(y_test, y_pred_xgb),\n",
    "    \"precision\": precision_score(y_test, y_pred_xgb, zero_division=0),\n",
    "    \"recall\": recall_score(y_test, y_pred_xgb, zero_division=0),\n",
    "    \"f1\": f1_score(y_test, y_pred_xgb, zero_division=0),\n",
    "}\n",
    "\n",
    "# 4. Gradient Boosting (this is the one we finally use in deployment)\n",
    "gb = GradientBoostingClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=3,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "gb.fit(X_train, y_train)\n",
    "y_pred_gb = gb.predict(X_test)\n",
    "\n",
    "results[\"GradientBoosting\"] = {\n",
    "    \"accuracy\": accuracy_score(y_test, y_pred_gb),\n",
    "    \"precision\": precision_score(y_test, y_pred_gb, zero_division=0),\n",
    "    \"recall\": recall_score(y_test, y_pred_gb, zero_division=0),\n",
    "    \"f1\": f1_score(y_test, y_pred_gb, zero_division=0),\n",
    "}\n",
    "\n",
    "pd.DataFrame(results).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred_gb)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix – Gradient Boosting\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Classification report – Gradient Boosting:\")\n",
    "print(classification_report(y_test, y_pred_gb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = gb.feature_importances_\n",
    "feature_names = X_encoded.columns\n",
    "\n",
    "fi_df = pd.DataFrame({\n",
    "    \"feature\": feature_names,\n",
    "    \"importance\": importances\n",
    "}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "fi_df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of POC\n",
    "\n",
    "- Dataset: `fake_dataset.xlsx` with features such as:\n",
    "  - `platform`, `has_profile_pic`, `bio_length`, `followers`, `following`,\n",
    "    `follower_following_ratio`, `account_age_days`, `posts`, `posts_per_day`,\n",
    "    `caption_similarity_score`, `content_similarity_score`, `follow_unfollow_rate`,\n",
    "    `spam_comments_rate`, `generic_comment_rate`, `suspicious_links_in_bio`,\n",
    "    `verified`, `username_length`, `digits_count`, `digit_ratio`, `special_char_count`,\n",
    "    `repeat_char_count`, etc.\n",
    "- Target variable: **`is_fake`** (binary: real vs fake)\n",
    "\n",
    "### Modelling Approach in POC\n",
    "\n",
    "1. Performed basic cleaning:\n",
    "   - Dropped high-cardinality `username` and ID-like columns\n",
    "   - Filled missing numeric values with median\n",
    "   - Filled missing categorical values with mode\n",
    "2. One-hot encoded categorical fields (e.g., `platform`) using `pd.get_dummies`.\n",
    "3. Compared four supervised classifiers:\n",
    "   - Logistic Regression (with scaling)\n",
    "   - Random Forest\n",
    "   - XGBoost\n",
    "   - Gradient Boosting\n",
    "4. Evaluated using accuracy, precision, recall, F1-score, and confusion matrix.\n",
    "\n",
    "**Final Choice:**  \n",
    "Gradient Boosting gave a strong balance of performance and interpretability,  \n",
    "so it was selected as the final model family for deployment.  \n",
    "\n",
    "In the **deployment phase**, a more robust training script (`train_model.py`)  \n",
    "is used, which:\n",
    "- Builds a reusable preprocessing pipeline (imputation, scaling, encoding)\n",
    "- Saves the final model as `outputs/best_model.joblib`\n",
    "- Is consumed by the Streamlit app (`app_fake_checker.py`) for live predictions."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
